{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccc55f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fb01e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-E8SRJD9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2264ea70d90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Aggregate exercise').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e7bf12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(simpleData,schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4513028",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df_sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e7fea28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|Distinct_Salary_count|\n",
      "+---------------------+\n",
      "|                    6|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg = df.agg(approx_count_distinct(\"salary\").alias('Distinct_Salary_count'))\n",
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93d7ff10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|Distinct_Salary_count|\n",
      "+---------------------+\n",
      "|6                    |\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg_sql = spark.sql('select count(distinct(salary)) as Distinct_Salary_count from df_sql').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5551d68b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|collect_list                                                |\n",
      "+------------------------------------------------------------+\n",
      "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_salary_collect_list = df.agg(collect_list('salary').alias('collect_list'))\n",
    "df_salary_collect_list.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ae636e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|collect_list|\n",
      "+------------+\n",
      "|3000        |\n",
      "|4600        |\n",
      "|4100        |\n",
      "|3000        |\n",
      "|3000        |\n",
      "|3300        |\n",
      "|3900        |\n",
      "|3000        |\n",
      "|2000        |\n",
      "|4100        |\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_salary_collect_list = spark.sql('select salary as collect_list from df_sql').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b999428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|collect_set                         |\n",
      "+------------------------------------+\n",
      "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_salary_collect_set = df.agg(collect_set('salary').alias('collect_set'))\n",
    "df_salary_collect_set.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c799c95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|collect_set|\n",
      "+-----------+\n",
      "|4600       |\n",
      "|3000       |\n",
      "|4100       |\n",
      "|3300       |\n",
      "|3900       |\n",
      "|2000       |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_salary_collect_list = spark.sql('select distinct(salary) as collect_set from df_sql').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01c7ca8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|10   |\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_count = df.agg(count('salary').alias('count')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45c81679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|10   |\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_count_sql = spark.sql('select count(*) as count from df_sql').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c4aa680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|first_sal|\n",
      "+---------+\n",
      "|3000     |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_sal = df.agg(first('salary').alias('first_sal')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aae67277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|last_sal|\n",
      "+--------+\n",
      "|4100    |\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last_sal = df.agg(last('salary').alias('last_sal')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ed326a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|max |\n",
      "+----+\n",
      "|4600|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_sal = df.agg(max('salary').alias('max')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce43d230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|min |\n",
      "+----+\n",
      "|2000|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_sal = df.agg(min('salary').alias('min')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0790ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_sal = df.agg(min('salary')).collect()[0][\"min(salary)\"]\n",
    "min_sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "195db660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        Kumar| Marketing|  2000|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter = df.filter(col('salary') == min_sal)\n",
    "df_filter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96b5ff33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|Kumar        |Marketing |2000  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from df_sql where salary = (select min(salary) from df_sql)').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a021c391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum_salary|\n",
      "+----------+\n",
      "|34000     |\n",
      "+----------+\n",
      "\n",
      "root\n",
      " |-- sum_salary: long (nullable = true)\n",
      "\n",
      "+----------+\n",
      "|sum_salary|\n",
      "+----------+\n",
      "|     34000|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_sal = df.agg(sum('salary').alias('sum_salary'))\n",
    "sum_sal.show(truncate=False)\n",
    "sum_sal.printSchema()\n",
    "sum_sal_sql = spark.sql('select sum(salary) as sum_salary from df_sql').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b20587b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|Department|sum_depart_sal|\n",
      "+----------+--------------+\n",
      "|     Sales|         18800|\n",
      "|   Finance|         10200|\n",
      "| Marketing|          5000|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_department = df.groupBy('Department').sum('salary')\n",
    "group_department = group_department.withColumnRenamed('sum(salary)' , 'sum_depart_sal')\n",
    "group_department.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0931cccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|Department|sum_depart_sal|\n",
      "+----------+--------------+\n",
      "|     Sales|         18800|\n",
      "|   Finance|         10200|\n",
      "| Marketing|          5000|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_department_sql = spark.sql('select Department, sum(salary) as sum_depart_sal from df_sql group by 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa574e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-------+-------+-------+\n",
      "|Department|count_sal|Total_sal|min_sal|max_sal|avg_sal|\n",
      "+----------+---------+---------+-------+-------+-------+\n",
      "| Marketing|        2|     5000|   2000|   3000| 2500.0|\n",
      "|   Finance|        3|    10200|   3000|   3900| 3400.0|\n",
      "|     Sales|        5|    18800|   3000|   4600| 3760.0|\n",
      "+----------+---------+---------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_department =  df.groupBy('Department')\\\n",
    "                    .agg( \\\n",
    "                          count(col('salary')).alias('count_sal'), \\\n",
    "                          sum(col('salary')).alias('Total_sal'), \\\n",
    "                          min(col('salary')).alias('min_sal'), \\\n",
    "                          max(col('salary')).alias('max_sal'), \\\n",
    "                          avg(col('salary')).alias('avg_sal') \\\n",
    "                        ).orderBy(col('count_sal').asc())\n",
    "\n",
    "group_department.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "499defac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-------+-------+-------+\n",
      "|Department|count_sal|Total_sal|min_sal|max_sal|avg_sal|\n",
      "+----------+---------+---------+-------+-------+-------+\n",
      "|Marketing |2        |5000     |2000   |3000   |2500.0 |\n",
      "|Finance   |3        |10200    |3000   |3900   |3400.0 |\n",
      "|Sales     |5        |18800    |3000   |4600   |3760.0 |\n",
      "+----------+---------+---------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_department_sql = spark.sql('select Department, \\\n",
    "                                 count(salary) as count_sal, \\\n",
    "                                 sum(salary) as Total_sal, \\\n",
    "                                 min(salary) as min_sal, \\\n",
    "                                 max(salary) as max_sal, \\\n",
    "                                 avg(salary) as avg_sal \\\n",
    "                                 from df_sql \\\n",
    "                                 group by Department \\\n",
    "                                 order by count_sal\\\n",
    "                                 ')\n",
    "                                 \n",
    "group_department_sql.show(truncate=False)                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d77f65c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f107d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df_sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2086c22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+\n",
      "|state|department|Total_Sal|avg_Sal|\n",
      "+-----+----------+---------+-------+\n",
      "|NY   |Sales     |176000   |88000.0|\n",
      "|NY   |Finance   |162000   |81000.0|\n",
      "|NY   |Marketing |91000    |91000.0|\n",
      "+-----+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('state', 'department')\\\n",
    "        .agg(\\\n",
    "            sum('salary').alias('Total_Sal'), \\\n",
    "            avg('salary').alias('avg_Sal')).orderBy(col('Total_Sal').desc())\\\n",
    "            .where(col('state') == 'NY').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "70267264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+\n",
      "|state|department|Total_sal|avg_sal|\n",
      "+-----+----------+---------+-------+\n",
      "|   NY|     Sales|   176000|88000.0|\n",
      "|   NY|   Finance|   162000|81000.0|\n",
      "|   NY| Marketing|    91000|91000.0|\n",
      "+-----+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(' \\\n",
    "            select state, department,\\\n",
    "            sum(salary) as Total_sal,\\\n",
    "            avg(salary) as avg_sal \\\n",
    "            from df_sql \\\n",
    "            group by state, department\\\n",
    "            having state = \"NY\" \\\n",
    "            ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957beb06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
