{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "800672f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-E8SRJD9:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20fb01a0d90>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName('Join exercise').getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd794e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  Alice|  1|\n",
      "|    Bob|  2|\n",
      "|    Bob|  7|\n",
      "|Charlie|  3|\n",
      "|Charlie|  3|\n",
      "+-------+---+\n",
      "\n",
      "+-----+----------+\n",
      "| name|profession|\n",
      "+-----+----------+\n",
      "|Alice|  Engineer|\n",
      "|  Bob|    Doctor|\n",
      "|David| Scientist|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data for df1\n",
    "data1 = [(\"Alice\", 1), (\"Bob\", 2), (\"Bob\", 7), (\"Charlie\", 3),(\"Charlie\", 3)]\n",
    "columns1 = [\"name\", \"id\"]\n",
    "df1 = spark.createDataFrame(data=data1, schema=columns1)\n",
    "df1.show()\n",
    "\n",
    "\n",
    "# Sample data for df2\n",
    "data2 = [(\"Alice\", \"Engineer\"), (\"Bob\", \"Doctor\"), (\"David\", \"Scientist\")]\n",
    "columns2 = [\"name\", \"profession\"]\n",
    "df2 = spark.createDataFrame(data=data2, schema=columns2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4a24b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('df1')\n",
    "df2.createOrReplaceTempView('df2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aabaff9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----------+\n",
      "| name| id| name|profession|\n",
      "+-----+---+-----+----------+\n",
      "|Alice|  1|Alice|  Engineer|\n",
      "|  Bob|  2|  Bob|    Doctor|\n",
      "|  Bob|  7|  Bob|    Doctor|\n",
      "+-----+---+-----+----------+\n",
      "\n",
      "Time taken to complete the dataframe inner join is -6.343767166137695.\n"
     ]
    }
   ],
   "source": [
    "# Inner join: This joins both DataFrames on the identifier and retains only the common rows and columns\n",
    "start_time = time.time()\n",
    "\n",
    "inner_join = df1.join(df2, df1['name'] == df2['name'], how = 'inner')\n",
    "inner_join.show()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = start_time - end_time\n",
    "print(f'Time taken to complete the dataframe inner join is {elapsed_time}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24830517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----------+\n",
      "| name| id| name|profession|\n",
      "+-----+---+-----+----------+\n",
      "|Alice|  1|Alice|  Engineer|\n",
      "|  Bob|  2|  Bob|    Doctor|\n",
      "|  Bob|  7|  Bob|    Doctor|\n",
      "+-----+---+-----+----------+\n",
      "\n",
      "Time taken to complete the sql inner join is -6.351756811141968.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "inner_join_sql = spark.sql('select a.*, b.* from df1 a inner join df2 b on a.name = b.name')\n",
    "inner_join_sql.show()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = start_time - end_time\n",
    "print(f'Time taken to complete the sql inner join is {elapsed_time}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "73ecaaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----------+\n",
      "|   name| id| name|profession|\n",
      "+-------+---+-----+----------+\n",
      "|  Alice|  1|Alice|  Engineer|\n",
      "|    Bob|  2|  Bob|    Doctor|\n",
      "|    Bob|  7|  Bob|    Doctor|\n",
      "|Charlie|  3| null|      null|\n",
      "|Charlie|  3| null|      null|\n",
      "+-------+---+-----+----------+\n",
      "\n",
      "Time taken to complete the sql left join is -6.247480154037476.\n"
     ]
    }
   ],
   "source": [
    "# Left/Left outer join: A left join merges both the DataFrames on the\n",
    "# identifier and retains all the rows of the left-hand DataFrame and\n",
    "# matches any rows that are common with the right-hand DataFrame.\n",
    "# If there is no equivalent row in the right-hand DataFrame, Spark will\n",
    "# insert a null for all the columns.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "left_join = df1.join(df2, df1['name'] == df2['name'], how = 'left')\n",
    "left_join.show()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = start_time - end_time\n",
    "print(f'Time taken to complete the sql left join is {elapsed_time}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8edc9f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----------+\n",
      "|   name| id| name|profession|\n",
      "+-------+---+-----+----------+\n",
      "|  Alice|  1|Alice|  Engineer|\n",
      "|    Bob|  2|  Bob|    Doctor|\n",
      "|    Bob|  7|  Bob|    Doctor|\n",
      "|Charlie|  3| null|      null|\n",
      "|Charlie|  3| null|      null|\n",
      "+-------+---+-----+----------+\n",
      "\n",
      "Time taken to complete the sql left join is -6.123517036437988.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "left_join_sql = spark.sql('select a.*, b.* from df1 a left join df2 b on a.name = b.name')\n",
    "left_join_sql.show()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = start_time - end_time\n",
    "print(f'Time taken to complete the sql left join is {elapsed_time}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "df2536a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+----------+\n",
      "| name|  id| name|profession|\n",
      "+-----+----+-----+----------+\n",
      "|Alice|   1|Alice|  Engineer|\n",
      "|  Bob|   7|  Bob|    Doctor|\n",
      "|  Bob|   2|  Bob|    Doctor|\n",
      "| null|null|David| Scientist|\n",
      "+-----+----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Right/Right outer join: This merges both DataFrames on the\n",
    "# identifier and retains all the rows of the right-hand DataFrame and\n",
    "# matches any rows that are common with the left-hand DataFrame.\n",
    "# If there is no equivalent row in the left-hand DataFrame, Spark will\n",
    "# insert a null for all the columns.\n",
    "\n",
    "right_join = df1.join(df2, df1['name'] == df2['name'], how = 'right')\n",
    "right_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dfacfeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+----------+\n",
      "| name|  id| name|profession|\n",
      "+-----+----+-----+----------+\n",
      "|Alice|   1|Alice|  Engineer|\n",
      "|  Bob|   7|  Bob|    Doctor|\n",
      "|  Bob|   2|  Bob|    Doctor|\n",
      "| null|null|David| Scientist|\n",
      "+-----+----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rightt_join_sql = spark.sql('select a.*, b.* from df1 a right join df2 b on a.name = b.name')\n",
    "rightt_join_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3b08da85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----------+\n",
      "|   name|  id| name|profession|\n",
      "+-------+----+-----+----------+\n",
      "|  Alice|   1|Alice|  Engineer|\n",
      "|    Bob|   2|  Bob|    Doctor|\n",
      "|    Bob|   7|  Bob|    Doctor|\n",
      "|Charlie|   3| null|      null|\n",
      "|Charlie|   3| null|      null|\n",
      "|   null|null|David| Scientist|\n",
      "+-------+----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Full outer join: This joins both DataFrames on the identifier and\n",
    "# retains all the rows of both the left and right DataFrames that have\n",
    "# the same identifier. If there are no equivalent rows in either of the\n",
    "# DataFrames, Spark will insert a null for all the columns.\n",
    "\n",
    "full_join = df1.join(df2, df1['name'] == df2['name'], how = 'full')\n",
    "full_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f1225819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----------+\n",
      "|   name|  id| name|profession|\n",
      "+-------+----+-----+----------+\n",
      "|  Alice|   1|Alice|  Engineer|\n",
      "|    Bob|   2|  Bob|    Doctor|\n",
      "|    Bob|   7|  Bob|    Doctor|\n",
      "|Charlie|   3| null|      null|\n",
      "|Charlie|   3| null|      null|\n",
      "|   null|null|David| Scientist|\n",
      "+-------+----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_join_sql = spark.sql('select a.*, b.* from df1 a full join df2 b on a.name = b.name')\n",
    "full_join_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6ea2b426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|Charlie|  3|\n",
      "|Charlie|  3|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Left Anti join: This joins both the DataFrames on the identifier and retains all the rows of left-hand DataFrame that are \n",
    "# not present in the right-hand DataFrame. It also only retains the left-hand DataFrame schema..\n",
    "\n",
    "\n",
    "anti_join = df1.join(df2, on=\"name\", how=\"left_anti\")\n",
    "anti_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "850ec412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|Charlie|  3|\n",
      "|Charlie|  3|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anti_join_sql = spark.sql('select a.* from df1 a left join df2 b on a.name = b.name where b.name is NULL')\n",
    "anti_join_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0b6a1896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name| id|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "|  Bob|  7|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Left Semi join: This is similar to an inner join, except it would not yield the columns from the right-hand DataFrame..\n",
    "\n",
    "left_semi = df1.join(df2, on=\"name\", how=\"left_semi\")\n",
    "left_semi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "67868eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name| id|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "|  Bob|  7|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left_semi_sql = spark.sql('select a.* from df1 a inner join df2 b on a.name = b.name')\n",
    "left_semi_sql.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
